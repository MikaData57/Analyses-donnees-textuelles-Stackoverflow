{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"text-align:center;\"><img src=\"http://www.mf-data-science.fr/images/projects/intro.jpg\" style='width:100%; margin-left: auto; margin-right: auto; display: block;' /></div>","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color: #641E16\">Contexte</span>\nNous allons ici développer un algorithme de Machine Learning destiné à assigner automatiquement plusieurs tags pertinents à une question posée sur le célébre site Stack overflow.     \nCe programme s'adresse principalement aux nouveaux utilisateurs, afin de leur suggérer quelques tags relatifs à la question qu'ils souhaitent poser.\n\n### Les données sources\nLes données ont été cleanées dans le Notebook Kaggle [Stackoverflow questions - data cleaning](https://www.kaggle.com/michaelfumery/stackoverflow-questions-data-cleaning). Dans ce nettoyage ont par exemple été appliquées les techniques de stop words, suppression de la ponctuation et des liens, tokenisation, lemmatisation ...\n\n### Objectif de ce Notebook\nDans ce Notebook, nous allons traiter la partie **modélisation des données textuelles avec des modèles supervisés et non supervisés**.     \n\nTous les Notebooks du projet seront **versionnés dans Kaggle mais également dans un repo GitHub** disponible à l'adresse https://github.com/MikaData57/Analyses-donnees-textuelles-Stackoverflow","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#641E16\">Sommaire</span>\n1. [Preprocessing : Bag of Words / Tf-Idf](#section_1)\n2. [Modèles non supervisés](#section_2)     \n    2.1. [Modèle LDA](#section_2_1)     \n    2.2. [Modèle NMF](#section_2_2)     \n3. [Modèles supervisés](#section_3)     \n    3.1. [Régression logistique avec multi-labels](#section_3_1)      \n    3.2. [Modélisation avec RandomForest](#section_3_2)       \n    3.3. [Modèle RandomForest avec Classifier Chains](#section_3_3)       \n    3.4. [Réseau de neurones avec Keras](#section_3_4)       ","metadata":{}},{"cell_type":"code","source":"# Install package for PEP8 verification\n!pip install pycodestyle\n!pip install --index-url https://test.pypi.org/simple/ nbpep8","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:06:50.974133Z","iopub.execute_input":"2021-06-30T11:06:50.974609Z","iopub.status.idle":"2021-06-30T11:07:08.465006Z","shell.execute_reply.started":"2021-06-30T11:06:50.974499Z","shell.execute_reply":"2021-06-30T11:07:08.463341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Python libraries\nimport os\nimport warnings\nimport time\nimport numpy as np\nimport pandas as pd\nfrom ast import literal_eval\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import set_config\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import NMF\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.multioutput import ClassifierChain\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nimport pyLDAvis\nimport pyLDAvis.sklearn\nimport pyLDAvis.gensim_models as gensimvis\n\n#RNN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.backend import clear_session\nfrom keras import backend as K\n\n# Library for PEP8 standard\nfrom nbpep8.nbpep8 import pep8","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-30T11:07:08.469879Z","iopub.execute_input":"2021-06-30T11:07:08.470262Z","iopub.status.idle":"2021-06-30T11:07:17.376175Z","shell.execute_reply.started":"2021-06-30T11:07:08.470227Z","shell.execute_reply":"2021-06-30T11:07:17.375207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore', category=DeprecationWarning)\nplt.style.use('seaborn-whitegrid')\nsns.set_style(\"whitegrid\")\nset_config(display='diagram')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:07:17.377972Z","iopub.execute_input":"2021-06-30T11:07:17.37852Z","iopub.status.idle":"2021-06-30T11:07:17.387467Z","shell.execute_reply.started":"2021-06-30T11:07:17.378477Z","shell.execute_reply":"2021-06-30T11:07:17.386583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define path to data\npath = '../input/stackoverflow-questions-filtered-2011-2021/'\ndata = pd.read_csv(path+\"StackOverflow_questions_2009_2020_cleaned.csv\",\n                   sep=\";\", index_col=0,\n                   converters={\"Title\": literal_eval,\n                               \"Body\": literal_eval,\n                               \"Tags\": literal_eval})\ndata.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:07:17.389208Z","iopub.execute_input":"2021-06-30T11:07:17.389677Z","iopub.status.idle":"2021-06-30T11:07:24.605573Z","shell.execute_reply.started":"2021-06-30T11:07:17.389597Z","shell.execute_reply":"2021-06-30T11:07:24.604761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:07:24.607593Z","iopub.execute_input":"2021-06-30T11:07:24.607956Z","iopub.status.idle":"2021-06-30T11:07:24.616153Z","shell.execute_reply.started":"2021-06-30T11:07:24.607923Z","shell.execute_reply":"2021-06-30T11:07:24.614707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous allons également créer une variable `Full_doc` qui accueillera le document complet de chaque item (Title et Body) :","metadata":{}},{"cell_type":"code","source":"data[\"Full_doc\"] = data[\"Title\"] + data[\"Body\"]\ndata[\"Full_doc\"].head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:07:24.617955Z","iopub.execute_input":"2021-06-30T11:07:24.618315Z","iopub.status.idle":"2021-06-30T11:07:24.804578Z","shell.execute_reply.started":"2021-06-30T11:07:24.618285Z","shell.execute_reply":"2021-06-30T11:07:24.803323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#641E16\" id=\"section_1\">Preprocessing : Bag of Words / Tf-Idf</span>\n\nPour alimenter les modèles de machine learning, nous avons besoin de traiter des données numériques. Le modèle **Bag of Words** apprend un vocabulaire à partir de tous les documents, puis modélise chaque document en comptant le nombre de fois où chaque mot apparaît, convertissant donc les données textuelles en données numériques.\n\nNos données ayant déjà été cleanées et tokenisées dans le Notebook [stackoverflow-questions-data-cleaning](https://www.kaggle.com/michaelfumery/stackoverflow-questions-data-cleaning), nous allons initialiser l'algorithme du `CountVectorizer` sur les variables `Title` et `Body` *(X1 et X2)* sans preprocessing. Enfin, nous allons utiliser le module `TfidfVectorizer` de la librairie Scikit-Learn pour combiner le `CountVectorizer` et `TfidfTransformer`. Cela aura pour effet de pondérer la fréquence d'apparition des mots par un indicateur de similarité *(si ce mot est commun ou rare dans tous les documents)*. Dans cette partie, nous allons **éliminer les mots qui apparaissent dans plus de 60% des documents** (`max_df = 0.6`).\n\nla métrique tf-idf ***(Term-Frequency - Inverse Document Frequency)*** utilise comme indicateur de similarité l'inverse document frequency qui est l'inverse de la proportion de document qui contient le terme, à l'échelle logarithmique.\n\nPour préparer nos targets *(pour les modèles supervisés)*, nous allons utiliser `MultiLabelBinarizer` de Scikit-Learn puisque nos `Tags` sont multiples.","metadata":{}},{"cell_type":"code","source":"# Define X and y\nX = data[\"Full_doc\"]\ny = data[\"Tags\"]\n\n# Initialize the \"CountVectorizer\" TFIDF for Full_doc\nvectorizer = TfidfVectorizer(analyzer=\"word\",\n                             max_df=.6,\n                             min_df=0.005,\n                             tokenizer=None,\n                             preprocessor=' '.join,\n                             stop_words=None,\n                             lowercase=False)\n\nX_tfidf = vectorizer.fit_transform(X)\n\nprint(\"Shape of X for Full_doc: {}\".format(X_tfidf.shape))\n\n# Multilabel binarizer for targets\nmultilabel_binarizer = MultiLabelBinarizer()\nmultilabel_binarizer.fit(y)\ny_binarized = multilabel_binarizer.transform(y)\nprint(\"Shape of y: {}\".format(y_binarized.shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:07:24.806481Z","iopub.execute_input":"2021-06-30T11:07:24.80705Z","iopub.status.idle":"2021-06-30T11:07:28.166724Z","shell.execute_reply.started":"2021-06-30T11:07:24.80698Z","shell.execute_reply":"2021-06-30T11:07:28.165226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create train and test split (30%)\nX_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_binarized,\n                                                    test_size=0.3, random_state=8)\nprint(\"X_train shape : {}\".format(X_train.shape))\nprint(\"X_test shape : {}\".format(X_test.shape))\nprint(\"y_train shape : {}\".format(y_train.shape))\nprint(\"y_test shape : {}\".format(y_test.shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:07:28.170934Z","iopub.execute_input":"2021-06-30T11:07:28.171304Z","iopub.status.idle":"2021-06-30T11:07:28.219676Z","shell.execute_reply.started":"2021-06-30T11:07:28.17127Z","shell.execute_reply":"2021-06-30T11:07:28.218504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comme les matrices sont relativment importantes, nous allons **vérifier le nombre de cellules qui ne sont pas à 0** :","metadata":{}},{"cell_type":"code","source":"full_dense = X_tfidf.todense()\nprint(\"Full_doc sparsicity: {:.3f} %\"\\\n      .format(((full_dense > 0).sum()/full_dense.size)*100))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:07:28.221948Z","iopub.execute_input":"2021-06-30T11:07:28.222286Z","iopub.status.idle":"2021-06-30T11:07:28.843043Z","shell.execute_reply.started":"2021-06-30T11:07:28.22225Z","shell.execute_reply":"2021-06-30T11:07:28.841289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous constatons que cette mesure est meilleure pour la varaible englobant Title et Body *(Full_doc)*.","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#641E16\" id=\"section_2\">Modèles non supervisés</span>\n\n## <span id=\"section_2_1\">Modèle LDA</span>\nLDA, ou **Latent Derelicht Analysis** est un modèle probabiliste qui, pour obtenir des affectations de cluster, utilise deux valeurs de probabilité : $P(word | topics)$ et $P(topics | documents)$. Ces valeurs sont calculées sur la base d'une attribution aléatoire initiale, puis le calcul est répété pour chaque mot dans chaque document, pour décider de leur attribution de sujet. Dans cette méthode itérative, ces probabilités sont calculées plusieurs fois, jusqu'à la convergence de l'algorithme.\n\nNous allons entrainer 1 seul modèle basé sur la variable `Full_doc` en utilisant la librairie spécialisée **Gensim**. Pour cette partie, nous n'utiliserons pas le preprocessing TFIDF mais des fonctions propres aux méthodes Gensim.\n\nDans une première étape, le Bag of words est créé ainsi que la matrice de fréquence des termes dans les documents :","metadata":{}},{"cell_type":"code","source":"# Create dictionnary (bag of words)\nid2word = corpora.Dictionary(X)\nid2word.filter_extremes(no_below=4, no_above=0.6, keep_n=None)\n# Create Corpus \ntexts = X  \n# Term Document Frequency \ncorpus = [id2word.doc2bow(text) for text in texts]  \n# View \nprint(corpus[:1])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:07:28.844829Z","iopub.execute_input":"2021-06-30T11:07:28.845197Z","iopub.status.idle":"2021-06-30T11:07:35.677383Z","shell.execute_reply.started":"2021-06-30T11:07:28.845168Z","shell.execute_reply":"2021-06-30T11:07:35.676191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gensim crée un identifiant unique pour chaque mot du document puis mappe word_id et word_frequency. Exemple : (6,3) ci-dessus indique que word_id 6 apparaît 3 fois dans le document et ainsi de suite.      \nLes mots les plus fréquents ont ici aussi été filtrés grâce à la fonction `filter_extremes` réglée à 60% comme pour le Tfidf.\n\nPour voir quel mot correspond à un identifiant donné, il faut transmettre l'identifiant comme clé du dictionnaire. Exemple : id2word[4] :","metadata":{}},{"cell_type":"code","source":"[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:07:35.67904Z","iopub.execute_input":"2021-06-30T11:07:35.679378Z","iopub.status.idle":"2021-06-30T11:07:35.693003Z","shell.execute_reply.started":"2021-06-30T11:07:35.679345Z","shell.execute_reply":"2021-06-30T11:07:35.691375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous allons à présent entrainer le modèle LDA sur Full_doc puis afficher les métriques : \n- **Perplexity** : $(\\exp(-1 \\times \\text{log-likelihood})$ *(Log likelihood : Densité de vraisemblance)*\n- **Coherence Score** : Les mesures de cohérence de topics évaluent un seul topic en mesurant le degré de similitude sémantique entre les mots à score élevé dans ce dernier.       Pour en savoir plus sur ce Pipeline : [What is topic coherence ?](https://rare-technologies.com/what-is-topic-coherence/)","metadata":{}},{"cell_type":"code","source":"# Build LDA model\nfull_lda_model = gensim.models.ldamulticore\\\n                    .LdaMulticore(corpus=corpus,\n                                  id2word=id2word,\n                                  num_topics=20,\n                                  random_state=8,\n                                  per_word_topics=True,\n                                  workers=4)\n# Print Perplexity score\nprint('\\nPerplexity: ', full_lda_model.log_perplexity(corpus))\n\n#Print Coherence Score\ncoherence_model_lda = CoherenceModel(model=full_lda_model, \n                                     texts=texts, \n                                     dictionary=id2word, \n                                     coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:07:35.69472Z","iopub.execute_input":"2021-06-30T11:07:35.69505Z","iopub.status.idle":"2021-06-30T11:09:46.080748Z","shell.execute_reply.started":"2021-06-30T11:07:35.695018Z","shell.execute_reply":"2021-06-30T11:09:46.079504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"color:#641E16;\">Visualisation des résultats de LDA Gensim sur Full_doc avec 20 topics</span>","metadata":{}},{"cell_type":"code","source":"pyLDAvis.enable_notebook()\ngensimvis.prepare(full_lda_model, corpus, id2word)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:09:46.082575Z","iopub.execute_input":"2021-06-30T11:09:46.082939Z","iopub.status.idle":"2021-06-30T11:10:43.754322Z","shell.execute_reply.started":"2021-06-30T11:09:46.082902Z","shell.execute_reply":"2021-06-30T11:10:43.752892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"D'après les résulatas de cette modélisation LDA, il semble très **difficile de \"nommer\" les topics créés car les mots qui les composent sont très variés et sans fil conducteur clairement établi**. On voit cependant par exemple que le topic représenté par \"server\" englobe également \"database\", \"sql\", \"connection\" ou encore \"query\" ce qui est cohérent.\n\n### Amélioration du modèle LDA\n\nCependant, dans l'algoritmes LDA, nous avons fixé arbitrairement à 20 le paramètre `num_topics` qui représente le nombre de topics à créer. Afin de sélectionner le meilleur nombre de topics pour nos données, nous allons **itérer sur une fourchette de nombre de topics et tester le score de cohérence pour chaque modèle** :","metadata":{}},{"cell_type":"code","source":"# Iter LDA for best number of topics\ncoherence_test = []\nfor k in np.arange(1,90,10):\n    print(\"Fitting LDA for K = {}\".format(k))\n    start_time = time.time()\n    lda_model = gensim.models.ldamulticore\\\n                    .LdaMulticore(corpus=corpus,\n                                  id2word=id2word,\n                                  num_topics=k,\n                                  random_state=8,\n                                  per_word_topics=True,\n                                  workers=4)\n    coherence_model_lda = CoherenceModel(model=lda_model,\n                                         texts=texts,\n                                         dictionary=id2word,\n                                         coherence='c_v')\n    coherence_lda = coherence_model_lda.get_coherence()\n    end_time = time.time()\n    coherence_test.append((k, coherence_lda, (end_time - start_time)))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:10:43.756378Z","iopub.execute_input":"2021-06-30T11:10:43.756748Z","iopub.status.idle":"2021-06-30T11:21:31.315815Z","shell.execute_reply.started":"2021-06-30T11:10:43.756709Z","shell.execute_reply":"2021-06-30T11:21:31.31434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Affichons les scores des divers modèle :","metadata":{}},{"cell_type":"code","source":"# Create dataframe of results\ncoherence_test = pd.DataFrame(coherence_test,\n                              columns=[\"k\",\"coherence\",\"time\"])\n\n# Select best number of topics\nbest_nb_topics = coherence_test\\\n                    .loc[coherence_test.coherence.argmax(),\"k\"]\n\n# Plot results\nfig, ax1 = plt.subplots(figsize=(12,8))\nx = coherence_test[\"k\"]\ny1 = coherence_test[\"coherence\"]\ny2 = coherence_test[\"time\"]\n\nax1.plot(x, y1, label=\"Coherence score\")\nax1.axvline(x=best_nb_topics, color='r', alpha=.7,\n            linestyle='dashdot', label='Best param')\nax1.set_xlabel(\"Number of components\")\nax1.set_ylabel(\"Coherence score\")\n\nax2 = ax1.twinx()\nax2.plot(x, y2, label=\"Fit time\",\n         color='g', alpha=.5,\n         linestyle='--')\nax2.set_ylabel(\"Fitting time (s)\")\n\nplt.title(\"Choosing Optimal LDA Model\\n\",\n          color=\"#641E16\", fontsize=18)\nlegend = fig.legend(loc=1, bbox_to_anchor=(.92, .9))\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:21:31.319266Z","iopub.execute_input":"2021-06-30T11:21:31.319935Z","iopub.status.idle":"2021-06-30T11:21:31.983457Z","shell.execute_reply.started":"2021-06-30T11:21:31.319869Z","shell.execute_reply":"2021-06-30T11:21:31.982166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testons à présent le modèle avec le meilleur nombre théorique de topics pour l'afficher avec LDAvis :","metadata":{}},{"cell_type":"code","source":"# Best LDA visualization\n# Construire le modèle LDA\nbest_lda_model = gensim.models.ldamulticore\\\n                    .LdaMulticore(corpus=corpus,\n                                  id2word=id2word,\n                                  num_topics=best_nb_topics,\n                                  random_state=8,\n                                  per_word_topics=True,\n                                  workers=4)\ngensimvis.prepare(best_lda_model, corpus, id2word)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:21:31.985113Z","iopub.execute_input":"2021-06-30T11:21:31.985725Z","iopub.status.idle":"2021-06-30T11:23:08.181805Z","shell.execute_reply.started":"2021-06-30T11:21:31.985677Z","shell.execute_reply":"2021-06-30T11:23:08.18008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pour attribuer des Tags à chaque question sur ces modèles non-supervisés, nous allons **créer une matrice Topic/Tags** en réalisant une multiplication matricielle des matrices Document / Topic et Document / Tags.","metadata":{}},{"cell_type":"code","source":"# Calculate Document/topic matrix with Gensim\ndoc_topic = pd.DataFrame(best_lda_model.get_document_topics(corpus, minimum_probability=0))\nfor topic in doc_topic.columns:\n    doc_topic[topic] = doc_topic[topic].apply(lambda x : x[1])\n\nprint('document/tag : ', y_binarized.shape)\nprint('document/topic : ', doc_topic.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:23:08.183931Z","iopub.execute_input":"2021-06-30T11:23:08.184312Z","iopub.status.idle":"2021-06-30T11:24:21.529396Z","shell.execute_reply.started":"2021-06-30T11:23:08.184275Z","shell.execute_reply":"2021-06-30T11:24:21.528035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print documents / topic matrix\ndoc_topic.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:24:21.531178Z","iopub.execute_input":"2021-06-30T11:24:21.531608Z","iopub.status.idle":"2021-06-30T11:24:21.565394Z","shell.execute_reply.started":"2021-06-30T11:24:21.531562Z","shell.execute_reply":"2021-06-30T11:24:21.563996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A présent, créons la matrice Topic / Tags grâce aux probabilités obtenues :","metadata":{}},{"cell_type":"code","source":"# Matricial multiplication with Document / Topics transpose\ntopic_tag = np.matmul(doc_topic.T, y_binarized)\ntopic_tag.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:24:21.566869Z","iopub.execute_input":"2021-06-30T11:24:21.567176Z","iopub.status.idle":"2021-06-30T11:24:21.62115Z","shell.execute_reply.started":"2021-06-30T11:24:21.567147Z","shell.execute_reply":"2021-06-30T11:24:21.619958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_tag","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:24:21.623848Z","iopub.execute_input":"2021-06-30T11:24:21.624691Z","iopub.status.idle":"2021-06-30T11:24:21.727626Z","shell.execute_reply.started":"2021-06-30T11:24:21.624622Z","shell.execute_reply":"2021-06-30T11:24:21.72657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous obtenons donc une matrice dont les lignes représentent les Topics créés et les colonnes les Tags associés et leurs distribution. Nous allons donc **créer nos prédictions en prenant les** $\\large n$ **premiers tags associés aux topics** de chaque document :","metadata":{}},{"cell_type":"code","source":"y_results = pd.DataFrame(y)\ny_results[\"best_topic\"] = doc_topic.idxmax(axis=1).values\ny_results[\"nb_tags\"] = y_results[\"Tags\"].apply(lambda x : len(x))\n\ndf_y_bin = pd.DataFrame(y_binarized)\ndf_dict = dict(\n    list(\n        df_y_bin.groupby(df_y_bin.index)\n    )\n)\n\ntags_num = []\nfor k, v in df_dict.items():\n    check = v.columns[(v == 1).any()]\n    tags_num.append(check.to_list())\n\ny_results[\"y_true\"] = tags_num\ny_results.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:24:21.729594Z","iopub.execute_input":"2021-06-30T11:24:21.730363Z","iopub.status.idle":"2021-06-30T11:25:43.223008Z","shell.execute_reply.started":"2021-06-30T11:24:21.730312Z","shell.execute_reply":"2021-06-30T11:25:43.221903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select predicted tags in Topics / Tags matrix\nlist_tag = []\nfor row in y_results.itertuples():\n    nb_tags = row.nb_tags\n    best_topic = row.best_topic\n    row_tags = list(topic_tag.iloc[best_topic].sort_values(ascending=False)[0:nb_tags].index)\n    list_tag.append(row_tags)\n    \ny_results[\"y_pred\"] = list_tag\ny_results.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:25:43.224725Z","iopub.execute_input":"2021-06-30T11:25:43.225045Z","iopub.status.idle":"2021-06-30T11:26:06.352335Z","shell.execute_reply.started":"2021-06-30T11:25:43.225015Z","shell.execute_reply":"2021-06-30T11:26:06.351209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous allons tester plusieurs métriques sur ce modèle LDA :\n- Accuracy score :\n- F1 score :\n- Jaccard similarity score : \n- Recall :\n- Precision :","metadata":{}},{"cell_type":"code","source":"def metrics_score(model, df, y_true, y_pred):\n    if(df is not None):\n        temp_df = df\n    else:\n        temp_df = pd.DataFrame(index=[\"Accuracy\", \"F1\", \"Jaccard\", \"Recall\", \"Precision\"],\n                               columns=[model])\n        \n    scores = []\n    scores.append(metrics.accuracy_score(y_true, y_pred))\n    scores.append(metrics.f1_score(y_pred, y_true, average='weighted'))\n    scores.append(metrics.jaccard_score(y_true, y_pred, average='weighted'))\n    scores.append(metrics.recall_score(y_true, y_pred, average='weighted'))\n    scores.append(metrics.precision_score(y_true, y_pred, average='weighted'))\n    temp_df[model] = scores\n    \n    return temp_df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:26:06.355796Z","iopub.execute_input":"2021-06-30T11:26:06.356159Z","iopub.status.idle":"2021-06-30T11:26:06.366313Z","shell.execute_reply.started":"2021-06-30T11:26:06.356121Z","shell.execute_reply":"2021-06-30T11:26:06.365028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"metrics_score(\"LDA\", df=None, \n              y_true=y_results.y_true, \n              y_pred=y_results.y_pred)","metadata":{}},{"cell_type":"markdown","source":"On remarque ici que la modélisation non supervisée avec LDA n'est pas adaptée. En effet, le meilleur nombre de topics se situerait à 31, mais **l'algorithme ne parvient pas a établir de groupes bien distincts**. Un certain nombre de topics sont très regroupés et donc représentés par les mêmes termes.\n\nNous allons donc tester une seconde modélisation non supervisée.\n\n## <span id=\"section_2_2\">Modèle NMF</span>\n<details>\n  <summary style=\"color:blue;\">Explication du modèle</summary>\n  \n  ## NMF\n  La factorisation matricielle non négative *(**N**on-negative **M**atrix **F**actorization)* est un modèle linéaire-algéabrique, qui factorise des vecteurs de grande dimension dans une représentation de faible dimension. Similaire à l'analyse en composantes principales *(PCA)*, NMF profite du fait que **les vecteurs sont non négatifs**. En les factorisant dans la forme de dimension inférieure, NMF force les coefficients à être également non négatifs.<br/><br/>\n    Prenons une matrice d'origine $A$, nous pouvons obtenir deux matrices $W$ et $H$, telles que $A = WH$. NMF a une propriété de clustering, telle que $W$ et $H$ représentent les informations suivantes sur $A$ :\n    <ul><li>$A$ (Matrice Document-word) : Matrice qui contient \"quels mots apparaissent dans quels documents\".</li>\n    <li>$W$ (Vecteurs de base) : Topics découverts à partir des documents.</li>\n    <li>$H$ (Matrice de coefficients) : les poids pour les topics dans chaque document.</li></ul><br/>\n    Nous calculons $W$ et $H$ en optimisant sur une **fonction objectif**, en mettant à jour à la fois $W$ et $H$ de manière itérative jusqu'à convergence.</br></br>\n    $$\\large \\frac{1}{2} ||A - WH||^2_F = \\sum_{i=1}^{n} \\sum_{j=1}^{m} (A_{ij} - (WH)_{ij})^2$$</br>\n    Dans cette fonction objectif, nous mesurons l'erreur de reconstruction entre A et le produit de ses facteurs W et H, en fonction de la distance euclidienne. Les valeurs mises à jour sont calculées dans des opérations parallèles, et en utilisant les nouveaux W et H, nous recalculons l'erreur de reconstruction, en répétant ce processus jusqu'à la convergence.\n</details>","metadata":{}},{"cell_type":"markdown","source":"Le modèle **NMF ne peut malheureusement pas être scoré**. Nous allons donc nous baser sur les résultats de la LDA pour déterminer un nombre correct de composants. Ici, nous prendrons **31 topics** pour avoir un bon compromis \"temps d'entrainement\" / précision et utiliserons les matrices Tfidf créées lors du preprocessing.","metadata":{}},{"cell_type":"code","source":"def plot_top_words(model, feature_names, n_top_words, nb_topic_plot, title):\n    \"\"\"Function for displaying the plots of the \n    best x words representative of the categories of NMF.\n\n    Parameters\n    ----------------------------------------\n    model : NMF model\n        Fitted model of NMF to plot\n    feature_names : array\n        Categories result of the vectorizer (TFIDF ...)\n    n_top_words : int\n        Number of words for each topic.\n    title : string\n        Title of the plot.\n    ----------------------------------------\n    \"\"\"\n    rows = int(nb_topic_plot/6)\n    fig, axes = plt.subplots(rows, 6, \n                             figsize=(30, rows*10), \n                             sharex=True)\n    axes = axes.flatten()\n    for topic_idx, topic in enumerate(model.components_):\n        if(topic_idx < nb_topic_plot):\n            top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n            top_features = [feature_names[i] for i in top_features_ind]\n            weights = topic[top_features_ind]\n\n            ax = axes[topic_idx]\n            bartopic = ax.barh(top_features, weights, height=0.7)\n            bartopic[0].set_color('#f48023')\n            ax.set_title(f'Topic {topic_idx +1}',\n                         fontdict={'fontsize': 30})\n            ax.invert_yaxis()\n            ax.tick_params(axis='both', which='major', labelsize=20)\n            for i in 'top right left'.split():\n                ax.spines[i].set_visible(False)\n            fig.suptitle(title, fontsize=36, color=\"#641E16\")\n\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:26:06.368007Z","iopub.execute_input":"2021-06-30T11:26:06.368843Z","iopub.status.idle":"2021-06-30T11:26:06.38487Z","shell.execute_reply.started":"2021-06-30T11:26:06.368775Z","shell.execute_reply":"2021-06-30T11:26:06.383796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define number of topics to test\nn_topics = best_nb_topics\n\nprint(\"-\"*50)\nprint(\"Start NMF fitting on Full_doc ...\")\nprint(\"-\" * 50)\nstart_time = time.time()\n# Initializing the NMF\nfull_nmf = NMF(n_components=n_topics,\n               init='nndsvd',\n               random_state=8)\n\n# Fit NMF on Body vectorized\nfull_nmf.fit(X_tfidf)\n\nexec_time = time.time() - start_time\nprint(\"End of training :\")\nprint(\"Execution time : {:.2f}s\".format(exec_time))\nprint(\"-\" * 50)\n\n# Plot the 12 first topics\nff_feature_names = vectorizer.get_feature_names()\nplot_top_words(full_nmf, ff_feature_names, 20, 12,\n               'Topics in NMF model for Full_doc')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:26:06.386312Z","iopub.execute_input":"2021-06-30T11:26:06.386672Z","iopub.status.idle":"2021-06-30T11:26:54.873342Z","shell.execute_reply.started":"2021-06-30T11:26:06.386618Z","shell.execute_reply":"2021-06-30T11:26:54.87208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**La modélisation avec NMF nous apporte des catégories aussi lisibles que celles de l'algorithme LDA**. 1 mot est toujours beaucoup plus représentaif de cette catégorie mais les regroupements sont globalement cohérents. Un topic par exemple illustre bien les sujets liés SQL, aux requêtes, un second traite les sujets liés aux dictionnaires ...\n\nEn revanche, les topics générés restent très généraux et ne permettent pas une catégorisation cohérente pour notre problème d'auto-tagging. Nous allons donc tester des modèles supervisés.","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#641E16\" id=\"section_3\">Modèles supervisés</span>\n\n## <span id=\"section_3_1\">Régression logistique avec multi-labels</span>\nNous avons déjà réalisé quelques Notebook traitant de la régression logistique : c'est une technique prédictive. Elle vise à construire un modèle permettant de prédire / expliquer les valeurs prises par une variable cible qualitative à partir d’un ensemble de variables explicatives quantitatives ou qualitatives encodées.\n\nPour cette partie sur les modélisations supervisées, nous allons utiliser la variable `Full_doc` qui regroupe le Title et le Body puis créer un Pipeline qui ne pourra pas inclure la transformation de notre variable cible *(MultiLabelBinarizer ne fonctionne pas dans les Pipeline SKlearn)*.","metadata":{}},{"cell_type":"code","source":"# Initialize Logistic Regression with OneVsRest\nparam_logit = {\"estimator__C\": [100, 10, 1.0, 0.1],\n               \"estimator__penalty\": [\"l1\", \"l2\"],\n               \"estimator__dual\": [False],\n               \"estimator__solver\": [\"liblinear\"]}\n\nmulti_logit_cv = GridSearchCV(OneVsRestClassifier(LogisticRegression()),\n                              param_grid=param_logit,\n                              n_jobs=-1,\n                              cv=5,\n                              scoring=\"f1_weighted\",\n                              return_train_score = True,\n                              refit=True)\nmulti_logit_cv.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:26:54.874793Z","iopub.execute_input":"2021-06-30T11:26:54.875122Z","iopub.status.idle":"2021-06-30T11:33:55.886845Z","shell.execute_reply.started":"2021-06-30T11:26:54.875089Z","shell.execute_reply":"2021-06-30T11:33:55.885567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logit_cv_results = pd.DataFrame.from_dict(multi_logit_cv.cv_results_)\nprint(\"-\"*50)\nprint(\"Best params for Logistic Regression\")\nprint(\"-\" * 50)\nlogit_best_params = multi_logit_cv.best_params_\nprint(logit_best_params)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:33:55.888611Z","iopub.execute_input":"2021-06-30T11:33:55.888964Z","iopub.status.idle":"2021-06-30T11:33:55.899066Z","shell.execute_reply.started":"2021-06-30T11:33:55.888928Z","shell.execute_reply":"2021-06-30T11:33:55.898169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logit_cv_results[logit_cv_results[\"params\"]==logit_best_params]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:33:55.90022Z","iopub.execute_input":"2021-06-30T11:33:55.900639Z","iopub.status.idle":"2021-06-30T11:33:55.94237Z","shell.execute_reply.started":"2021-06-30T11:33:55.900608Z","shell.execute_reply":"2021-06-30T11:33:55.941523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous pouvons maintenant **réaliser les prédictions avec le modèle de régression logistique sur le jeu de test** pour pouvoir les comparer avec le jeu y_test.","metadata":{}},{"cell_type":"code","source":"# Predict\ny_test_predicted_labels_tfidf = multi_logit_cv.predict(X_test)\n\n# Inverse transform\ny_test_pred_inversed = multilabel_binarizer.inverse_transform(y_test_predicted_labels_tfidf)\ny_test_inversed = multilabel_binarizer.inverse_transform(y_test)\n\nprint(\"-\"*50)\nprint(\"Print 5 first predicted Tags vs true Tags\")\nprint(\"-\" * 50)\nprint(\"Predicted:\", y_test_pred_inversed[0:5])\nprint(\"True:\", y_test_inversed[0:5])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:33:55.943633Z","iopub.execute_input":"2021-06-30T11:33:55.944208Z","iopub.status.idle":"2021-06-30T11:33:56.20588Z","shell.execute_reply.started":"2021-06-30T11:33:55.944173Z","shell.execute_reply":"2021-06-30T11:33:56.204309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Puis nous **calculons les diverses métriques sur le meilleur modèle** de régression logistique :","metadata":{}},{"cell_type":"code","source":"df_metrics_compare = metrics_score(\"Logit\", df=None, \n              y_true = y_test, \n              y_pred = y_test_predicted_labels_tfidf)\ndf_metrics_compare","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:33:56.20739Z","iopub.execute_input":"2021-06-30T11:33:56.207918Z","iopub.status.idle":"2021-06-30T11:33:56.815015Z","shell.execute_reply.started":"2021-06-30T11:33:56.20787Z","shell.execute_reply":"2021-06-30T11:33:56.813566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span id=\"section_3_2\">Modélisation avec RandomForest</span>","metadata":{}},{"cell_type":"code","source":"# Initialize RandomForest with OneVsRest\nparam_rfc = {\"estimator__max_depth\": [5, 25, 50],\n             \"estimator__min_samples_leaf\": [1, 5, 10],\n             \"estimator__class_weight\": [\"balanced\"]}\n\nmulti_rfc_cv = GridSearchCV(OneVsRestClassifier(RandomForestClassifier()),\n                                  param_grid=param_rfc,\n                                  n_jobs=-1,\n                                  cv=2,\n                                  scoring=\"f1_weighted\",\n                                  return_train_score = True,\n                                  refit=True,\n                                  verbose=3)\n# Fit on Sample data\nmulti_rfc_cv.fit(X_train[0:7000], y_train[0:7000])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:33:56.816239Z","iopub.execute_input":"2021-06-30T11:33:56.816545Z","iopub.status.idle":"2021-06-30T11:37:54.734814Z","shell.execute_reply.started":"2021-06-30T11:33:56.816516Z","shell.execute_reply":"2021-06-30T11:37:54.73333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc_cv_results = pd.DataFrame.from_dict(multi_rfc_cv.cv_results_)\nprint(\"-\"*50)\nprint(\"Best params for RandomForestClassifier\")\nprint(\"-\" * 50)\nrfc_best_params = multi_rfc_cv.best_params_\nprint(rfc_best_params)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:37:54.736258Z","iopub.execute_input":"2021-06-30T11:37:54.736572Z","iopub.status.idle":"2021-06-30T11:37:54.746231Z","shell.execute_reply.started":"2021-06-30T11:37:54.736541Z","shell.execute_reply":"2021-06-30T11:37:54.744986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc_best_params_ok = {}\nfor k, v in rfc_best_params.items():\n    rfc_best_params_ok[k.replace(\"estimator__\",\"\")] = v","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:37:54.74784Z","iopub.execute_input":"2021-06-30T11:37:54.748184Z","iopub.status.idle":"2021-06-30T11:37:54.758707Z","shell.execute_reply.started":"2021-06-30T11:37:54.748151Z","shell.execute_reply":"2021-06-30T11:37:54.75744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Refit RandomForestClassifier best_params with full dataset\nrfc_final_model = OneVsRestClassifier(RandomForestClassifier(**rfc_best_params_ok))\nrfc_final_model.fit(X_train, y_train)\n\n# Predict\ny_test_predicted_labels_tfidf_rfc = rfc_final_model.predict(X_test)\n\n# Inverse transform\ny_test_pred_inversed_rfc = multilabel_binarizer.inverse_transform(y_test_predicted_labels_tfidf_rfc)\n\nprint(\"-\"*50)\nprint(\"Print 5 first predicted Tags vs true Tags\")\nprint(\"-\" * 50)\nprint(\"Predicted:\", y_test_pred_inversed_rfc[0:5])\nprint(\"True:\", y_test_inversed[0:5])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:37:54.760283Z","iopub.execute_input":"2021-06-30T11:37:54.760641Z","iopub.status.idle":"2021-06-30T11:47:17.279039Z","shell.execute_reply.started":"2021-06-30T11:37:54.760609Z","shell.execute_reply":"2021-06-30T11:47:17.277486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_metrics_compare = metrics_score(\"RandomForest\", df=df_metrics_compare, \n              y_true = y_test, \n              y_pred = y_test_predicted_labels_tfidf_rfc)\ndf_metrics_compare","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:47:17.280413Z","iopub.execute_input":"2021-06-30T11:47:17.280749Z","iopub.status.idle":"2021-06-30T11:47:17.908768Z","shell.execute_reply.started":"2021-06-30T11:47:17.280716Z","shell.execute_reply":"2021-06-30T11:47:17.907447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Les métriques sur le modèle RandomForest sont moins bonnes mais semblent cependant plus cohérente avec les données, ce d'autant que les métriques Jaccard et F1 sont proches. D'autre part, nous pouvons **vérifier le nombre de lignes dont les Tags ne sont pas prédit** afin de voir si l'un des modèles est meilleur :","metadata":{}},{"cell_type":"code","source":"Tags_per_row_lr = y_test_predicted_labels_tfidf.sum(axis=1)\nnull_rate_lr = round(((Tags_per_row_lr.size - np.count_nonzero(Tags_per_row_lr))\n                      /Tags_per_row_lr.size)*100,2)\nTags_per_row_rfc = y_test_predicted_labels_tfidf_rfc.sum(axis=1)\nnull_rate_rfc = round(((Tags_per_row_rfc.size - np.count_nonzero(Tags_per_row_rfc))\n                       /Tags_per_row_rfc.size)*100,2)\nprint(\"-\"*50)\nprint(\"Percentage of non tagged question for each model\")\nprint(\"-\" * 50)\nprint(\"Logistic Regression: {}%\".format(null_rate_lr))\nprint(\"Random Forest: {}%\".format(null_rate_rfc))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:20:58.251916Z","iopub.execute_input":"2021-06-30T12:20:58.252814Z","iopub.status.idle":"2021-06-30T12:20:58.267507Z","shell.execute_reply.started":"2021-06-30T12:20:58.25275Z","shell.execute_reply":"2021-06-30T12:20:58.266081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Le Random Forest semble donc plus approprié à notre programme d'auto-tagging sur les données Stackoverflow. Nous allons à présent **tester ce modèle RandomForest avec Classifier Chains** pour remplacer la méthode One versus rest :\n\n## <span id=\"section_3_3\">Modèle RandomForest avec Classifier Chains</span>\n\nAvec la méthode ClassifierChains, chaque modèle fait une prédiction dans l'ordre spécifié en utilisant toutes les fonctionnalités disponibles fournies au modèle mais également les prédictions des modèles précédents.","metadata":{}},{"cell_type":"code","source":"rfc_base_model = RandomForestClassifier(**rfc_best_params_ok)\nchain = ClassifierChain(rfc_base_model, order='random')\nchain.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:29:04.371034Z","iopub.execute_input":"2021-06-30T12:29:04.372277Z","iopub.status.idle":"2021-06-30T12:38:06.512664Z","shell.execute_reply.started":"2021-06-30T12:29:04.372216Z","shell.execute_reply":"2021-06-30T12:38:06.511049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict\ny_test_predicted_labels_tfidf_chain = chain.predict(X_test)\n\n# Inverse transform\ny_test_pred_inversed_chain = multilabel_binarizer.inverse_transform(y_test_predicted_labels_tfidf_chain)\n\nprint(\"-\"*50)\nprint(\"Print 5 first predicted Tags vs true Tags\")\nprint(\"-\" * 50)\nprint(\"Predicted:\", y_test_pred_inversed_chain[0:5])\nprint(\"True:\", y_test_inversed[0:5])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:45:20.054017Z","iopub.execute_input":"2021-06-30T12:45:20.054676Z","iopub.status.idle":"2021-06-30T12:45:38.751704Z","shell.execute_reply.started":"2021-06-30T12:45:20.054545Z","shell.execute_reply":"2021-06-30T12:45:38.750432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tags_per_row_chain = y_test_predicted_labels_tfidf_chain.sum(axis=1)\nnull_rate_chain = round(((Tags_per_row_chain.size - np.count_nonzero(Tags_per_row_chain))\n                       /Tags_per_row_chain.size)*100,2)\nprint(\"-\"*50)\nprint(\"Percentage of non tagged question for chain model\")\nprint(\"-\" * 50)\nprint(\"RandomForest with Classifier Chains: {}%\".format(null_rate_chain))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:50:14.459915Z","iopub.execute_input":"2021-06-30T12:50:14.460773Z","iopub.status.idle":"2021-06-30T12:50:14.469617Z","shell.execute_reply.started":"2021-06-30T12:50:14.460727Z","shell.execute_reply":"2021-06-30T12:50:14.468449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_metrics_compare = metrics_score(\"RFC Chains\", df=df_metrics_compare, \n              y_true = y_test, \n              y_pred = y_test_predicted_labels_tfidf_chain)\ndf_metrics_compare","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:51:17.467424Z","iopub.execute_input":"2021-06-30T12:51:17.467894Z","iopub.status.idle":"2021-06-30T12:51:18.324471Z","shell.execute_reply.started":"2021-06-30T12:51:17.467859Z","shell.execute_reply":"2021-06-30T12:51:18.322936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Le modèle RandomForest avec Classifier Chains offre des métriques similaires au modèle avec OneVsRest mais le **taux de remplissage des valeurs prédites est encore meilleur**.\n\nNous allons à présent tester un dernier modèle d'apprentissage profond avec Keras et un réseau de neurones simple.","metadata":{}},{"cell_type":"markdown","source":"## <span id=\"section_3_4\">Réseau de neurones avec Keras</span>","metadata":{}},{"cell_type":"code","source":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\ndef jaccard_m(y_true, y_pred, smooth=100):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n    return (1 - jac) * smooth","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:55:01.684864Z","iopub.execute_input":"2021-06-30T12:55:01.685417Z","iopub.status.idle":"2021-06-30T12:55:01.699985Z","shell.execute_reply.started":"2021-06-30T12:55:01.68537Z","shell.execute_reply":"2021-06-30T12:55:01.69883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous allons définir une fonction pour constuire le réseau de neurones assez simple. RNN avec une couche cachée et complétement connectée. Nous utiliserons également un Dropout pour éviter le sur-apprentissage.","metadata":{}},{"cell_type":"code","source":"def build_rnn(input_dim, hidden_neurons, output_dim):\n    \"\"\"\n    Construct a Keras model which will be used to \n    fit/predict in SKlearn pipeline.\n    \"\"\"\n    # Create brain\n    model = Sequential()\n    model.add(layers.Dense(hidden_neurons, input_dim=input_dim, activation='relu'))\n    model.add(layers.Dropout(0.2))\n    model.add(layers.Dense(hidden_neurons, input_dim=input_dim, activation='relu'))\n    model.add(layers.Dropout(0.1))\n    model.add(layers.Dense(output_dim, activation='sigmoid'))\n    \n    # Compile model\n    model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy', recall_m, precision_m, f1_m, jaccard_m])\n    model.summary()\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-30T13:47:24.280826Z","iopub.execute_input":"2021-06-30T13:47:24.281401Z","iopub.status.idle":"2021-06-30T13:47:24.290476Z","shell.execute_reply.started":"2021-06-30T13:47:24.281366Z","shell.execute_reply":"2021-06-30T13:47:24.288837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clear_session()\n\nmodel_params = {\n    'build_fn': build_rnn,\n    'input_dim': X_train.shape[1],\n    'hidden_neurons': 150,\n    'output_dim': y_train.shape[1],\n    'epochs': 20,\n    'batch_size': 256,\n    'verbose': 0,\n    'validation_data': (X_test.toarray(), y_test),\n    'shuffle': True}\n\nrnn_model = KerasClassifier(**model_params)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T13:47:37.49034Z","iopub.execute_input":"2021-06-30T13:47:37.490722Z","iopub.status.idle":"2021-06-30T13:47:37.645404Z","shell.execute_reply.started":"2021-06-30T13:47:37.490692Z","shell.execute_reply":"2021-06-30T13:47:37.643606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = rnn_model.fit(X_train.toarray(), y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T13:48:10.868353Z","iopub.execute_input":"2021-06-30T13:48:10.868792Z","iopub.status.idle":"2021-06-30T13:48:46.934158Z","shell.execute_reply.started":"2021-06-30T13:48:10.868755Z","shell.execute_reply":"2021-06-30T13:48:46.933217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the model\nscores = rnn_model.score(X_test.toarray(), y_test)\nscores","metadata":{"execution":{"iopub.status.busy":"2021-06-30T13:32:05.431093Z","iopub.execute_input":"2021-06-30T13:32:05.431448Z","iopub.status.idle":"2021-06-30T13:32:05.960408Z","shell.execute_reply.started":"2021-06-30T13:32:05.431417Z","shell.execute_reply":"2021-06-30T13:32:05.959519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_accuracy = history.history.get('accuracy', [])\ntrain_f1 = history.history.get('f1_m', [])\nval_accuracy = history.history.get('val_accuracy', [])\nval_f1 = history.history.get('val_f1_m', [])\n\nfig, axes = plt.subplots(1, 2, figsize=(25, 8))\naxes[0].plot(np.arange(0,20,1),\n             train_accuracy,\n             label=\"Train\")\naxes[0].plot(np.arange(0,20,1),\n             val_accuracy,\n             linestyle='--', color='g', alpha=.7,\n             label=\"Validation\")\naxes[0].set_xticks(np.arange(0,20,5))\naxes[0].set_xlabel(\"Epochs\")\naxes[0].set_ylabel(\"Accuracy score\")\naxes[0].set_title('Model accuracy through epochs',\n                  color='#f48023', fontweight='bold')\naxes[0].legend(loc=4)\n\naxes[1].plot(np.arange(0,20,1),\n             train_f1, label=\"Train\")\naxes[1].plot(np.arange(0,20,1),\n             val_f1,\n             linestyle='--', color='g', alpha=.7,\n             label=\"Validation\")\naxes[1].set_xticks(np.arange(0,20,5))\naxes[1].set_xlabel(\"Epochs\")\naxes[1].set_ylabel(\"F1 score\")\naxes[1].set_title('Model F1 score through epochs',\n                  color='#f48023', fontweight='bold')\naxes[1].legend(loc=4)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T13:32:05.961872Z","iopub.execute_input":"2021-06-30T13:32:05.962289Z","iopub.status.idle":"2021-06-30T13:32:06.345168Z","shell.execute_reply.started":"2021-06-30T13:32:05.962258Z","shell.execute_reply":"2021-06-30T13:32:06.344019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnn_model.predict(X_test.toarray()).shape\n#np.argmax(rnn_model.predict(X_test.toarray()), axis=-1)\n#(rnn_model.predict(X_test.toarray()) > 0.5).astype(\"int32\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T13:32:06.346933Z","iopub.execute_input":"2021-06-30T13:32:06.347248Z","iopub.status.idle":"2021-06-30T13:32:06.827208Z","shell.execute_reply.started":"2021-06-30T13:32:06.347218Z","shell.execute_reply":"2021-06-30T13:32:06.825858Z"},"trusted":true},"execution_count":null,"outputs":[]}]}